{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Packages and Libraries ##\n",
    "\n",
    "# Web parcing, scraping, etc.\n",
    "import bs4 as bs # BeautifulSoup4 \n",
    "import urllib3\n",
    "import re\n",
    "import requests # HTTP parser\n",
    "import html5lib\n",
    "\n",
    "# DataFrames and math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Output related packages \n",
    "import pprint as pp\n",
    "import json\n",
    "\n",
    "# Progress bar and delaying requests \n",
    "from tqdm import tnrange, tqdm_notebook #progress bars\n",
    "from random import randint\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) \n",
    "\n",
    "# make it run on py2 and py3\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining I\n",
    "This  notebook is intended to perform the following processes:\n",
    "\n",
    "    1.1 Read-in news articles from newsAPI for a given date range, and up to five queries (passed as a list).\n",
    "\n",
    "    1.2 Extract features native to the articles (e.g. url).\n",
    "\n",
    "    1.3 Perform data cleanup and preprocessing.\n",
    "\n",
    "    1.4 Split dataset into n-csv-files for distrubuted computation or batching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Documentation__\n",
    "\n",
    "newsAPI (https://newsapi.org/) has limited documentation as to how their backend works. However, when developing this notebook the following website was frequently referenced -- as their documentation appears to closely match the behavior of newsAPI. \n",
    "\n",
    "https://docs.aylien.com/newsapi/#getting-started\n",
    "\n",
    "Moreover, here is the user agreement:\n",
    "\n",
    "https://newsapi.org/terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### **Begin Data Mining I:** Read-in NewsAPI feed for a given date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'newsapi.newsapi_client.NewsApiClient'>\n"
     ]
    }
   ],
   "source": [
    "### NEWSAPI RELATED ###\n",
    "# keys: \n",
    "#rhkey = '847446b32283474fafd2aec7f95e502b'\n",
    "#r1key  = '1da951c142304f7bab52ba8e3970495b'\n",
    "#r2key = '40d53e49ee3543a3b162e6a453e2e373'\n",
    "#m1key = '211fc2107848473e99c1f235b400a07f'\n",
    "#m2key = 'c0f99eab932d4cabb61c23239f3f482d'\n",
    "m3key = '658cd65a714349fdbb7e8dd6ce59e9c4'\n",
    "#m4key  = '8ba091b7a47b4c9a9162a83ca72eb1ca'\n",
    "#e1key  = '2bc85776a0c14af6b9937366ad683e2f'\n",
    "#e2key = '22e5c3a8f0ee4fa59aaf384ba9395a86'\n",
    "#e3key = 'c554f8fb27ca4be1862192b44ee4425d'\n",
    "\n",
    "\n",
    "# Install API \n",
    "#!pip install newsapi-python\n",
    "\n",
    "# Import Client\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "# Initialize Client (create object)\n",
    "news_api = NewsApiClient(api_key = m3key)\n",
    "print(type(news_api))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.1 Read-in news articles from newsAPI for a given date range__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_news**\n",
    "Function establishes values to be used for control of loop then calls functions used to extract news article data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(query, start, stop, sort, lang, article_count, page_count, init):\n",
    "    \"\"\"\n",
    "    control function for iterating over 100 pages of newsAPI's content \n",
    "    function then controls subordinate fuctions that extract ~100 articles per each page\n",
    "    \"\"\"\n",
    "    \n",
    "    import math\n",
    "    import time  \n",
    "    \n",
    "    # extract information about response file to ensure proper loop control\n",
    "    params = get_params(query, start, stop, sort, lang, article_count, page_count)\n",
    "\n",
    "    # variable referencing\n",
    "    status = params['status']\n",
    "    results = params['totalResults']\n",
    "\n",
    "    # Confirmation of data extraction\n",
    "    print(\"\\nVerify Read-in Process:\", status)\n",
    "    print(\"Number of Articles Correctly Read: \", results)\n",
    "    print(type(params), params.keys())\n",
    "           \n",
    "    # per page article extraction stop variable -- if number of articles is greater than number articles per page\n",
    "    loops = math.ceil(results/article_count)\n",
    "    \n",
    "    # batching control\n",
    "    begin = 0 + init\n",
    "    terminate = article_count + init\n",
    "    print(\"Total number of iterations (pages):\",loops)\n",
    "    \n",
    "    # check to ensure loop does not extract over 100pages*100endpoints=10000 articles\n",
    "    if loops < terminate:\n",
    "        terminate = loops\n",
    "        \n",
    "    print(\"Page range being extracted\", begin, terminate)\n",
    "    \n",
    "    if page_count == 'all' or article_count <  results:\n",
    "        print(\"\\n\\nExtracting News Data...\\n\")\n",
    "        full_df = pd.DataFrame()\n",
    "    \n",
    "        # function is called withinin while, is subject to number of pages available as a function of total no. articles\n",
    "        while begin < terminate:\n",
    "            begin += 1\n",
    "            page = begin  # for referencing clarity\n",
    "            \n",
    "            ### newsAPI has MAX HIT LIMIT of 60 per minute ### \n",
    "            time.sleep(1)   # delay of 1 second\n",
    "            print(\"Extracting Page\", page)\n",
    "\n",
    "            # call sequencial pages of articles and appends to df\n",
    "            df = news_data(query, start, stop, sort, lang, article_count, page)\n",
    "            full_df = full_df.append(df, ignore_index = True)\n",
    "            \n",
    "        print('Batch extraction completed:',begin,'of',terminate)\n",
    "        return(full_df)            \n",
    "    else:\n",
    "        # extracts articles assuming articles >= pages\n",
    "        print(\"Possible Invalid Parameters: Check values\")\n",
    "        brief_df = news_api.get_everything(q = query,\n",
    "                                          from_param= start,\n",
    "                                          to= stop,\n",
    "                                          sort_by= sort,\n",
    "                                          language= lang,\n",
    "                                          page_size= int(article_count)\n",
    "                                         )\n",
    "        return(brief_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_params**\n",
    "Function runs an initial newsAPI call, used to store values for controlling loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(query, start, stop, sort, lang, article_count, page_count):\n",
    "    \"\"\"\n",
    "    function accepts similar parameters to master function get_news.\n",
    "    get_params is used to extract parameters to be used in controlling other functions \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nExtracting Parameters for newsAPI...\\n\")\n",
    "    params = news_api.get_everything(q = query,\n",
    "                                     from_param= start,\n",
    "                                     to= stop,\n",
    "                                     sort_by= sort,\n",
    "                                     language= lang,\n",
    "                                     page_size= int(article_count)\n",
    "                                    )\n",
    "    \n",
    "    # Confirmation of data extraction\n",
    "    print(\"Read-in Status of Given Date Range:\", params['status'])\n",
    "    print(\"Number of Articles in Given Date Range: \", params['totalResults'])\n",
    "    \n",
    "    return(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **news_data**\n",
    "Function handles cases, and extracts values within 'articles'. Returns dataframe of contents: \n",
    "\n",
    "\n",
    "*Index(['author', 'description', 'publishedAt', 'source', 'title', 'url','urlToImage'],dtype='object')*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_data(query, start, stop, sort, lang, article_count, page):\n",
    "    \"\"\"\n",
    "    Principal data extraction function - can handle various relationships between no.pages and no.articles \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(page, int):\n",
    "        params = news_api.get_everything(q = query,\n",
    "                                         from_param= start,\n",
    "                                         to= stop,\n",
    "                                         sort_by= sort,\n",
    "                                         language= lang,\n",
    "                                         page_size= int(article_count),\n",
    "                                         page = int(page)\n",
    "                                        )\n",
    "    ########### if params['articles'] throws error ########### \n",
    "    # either endpoint limit was met (10000)                  #\n",
    "    # too many endpoint requests in a month (1000 per month) #\n",
    "    # change to new api key.                                 #\n",
    "    ##########################################################\n",
    "    return(pd.DataFrame(params['articles'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User provided parameters and function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#01/26/18 to 03/26/18\n",
    "query = 'Bitcoin'         # can handle a list of up to five search topics\n",
    "start = '2018-10-01'      # yyyy-mm-dd\n",
    "stop  = '2018-10-05'\n",
    "sort  = 'publishedAt'\n",
    "lang  = 'en'\n",
    "article_count = 100       # default is 20\n",
    "page_count = 'all'        # enter 1, 2, ... Notes: 'all' iterates over all articLes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "---\n",
    "#### __NOTE__\n",
    "\n",
    "Since newsAPI has a daily limit of endpoint requests (1000 per day), batching needs to be implemented.  The following cells controls the initialization for subsequent cells -- to ensure appropropriate, and sequential article extraction.\n",
    "\n",
    "Also, webAPI has a limit of 10,000 articles per key -- unless you want to pay for the monthly access.\n",
    "___\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------------------------------------------------------------------------- ##\n",
    "## KEY: Initialize accroding to batch number (i.e. n*100, where n is the batch) ##\n",
    "## USE: n > 0 ---- only once we can extract above 10000 articles\n",
    "n = 0\n",
    "init = n*100\n",
    "## ---------------------------------------------------------------------------- ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Parameters for newsAPI...\n",
      "\n",
      "Read-in Status of Given Date Range: ok\n",
      "Number of Articles in Given Date Range:  981\n",
      "\n",
      "Verify Read-in Process: ok\n",
      "Number of Articles Correctly Read:  981\n",
      "<class 'dict'> dict_keys(['status', 'totalResults', 'articles'])\n",
      "Total number of iterations (pages): 10\n",
      "Page range being extracted 0 10\n",
      "\n",
      "\n",
      "Extracting News Data...\n",
      "\n",
      "Extracting Page 1\n",
      "Extracting Page 2\n",
      "Extracting Page 3\n",
      "Extracting Page 4\n",
      "Extracting Page 5\n",
      "Extracting Page 6\n",
      "Extracting Page 7\n",
      "Extracting Page 8\n",
      "Extracting Page 9\n",
      "Extracting Page 10\n",
      "Batch extraction completed: 10 of 10\n"
     ]
    }
   ],
   "source": [
    "# object is the result of the following functions: 'get_params', 'get_news', and 'get_data'\n",
    "news = get_news(query, start, stop, sort, lang, article_count, page_count,init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore nested key/value pairs from newsAPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'content', 'description', 'publishedAt', 'source', 'title',\n",
       "       'url', 'urlToImage'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ENSURE API WAS ACCESSED ##\n",
    "# if news.keys includes 'author', 'description', etc., success!\n",
    "\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(news['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>description</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Hundeyin</td>\n",
       "      <td>Open source browser-based cryptocurrency walle...</td>\n",
       "      <td>Open source browser-based cryptocurrency walle...</td>\n",
       "      <td>2018-10-05T23:25:23Z</td>\n",
       "      <td>{'id': 'crypto-coins-news', 'name': 'Crypto Co...</td>\n",
       "      <td>Crypto Mining Giant Bitmain Acquires Bitcoin C...</td>\n",
       "      <td>https://www.ccn.com/crypto-mining-giant-bitmai...</td>\n",
       "      <td>https://www.ccn.com/wp-content/uploads/2018/07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chloe Aiello</td>\n",
       "      <td>Bitcoin is close to bottoming, and once it reb...</td>\n",
       "      <td>Bitcoin is close to bottoming, said Spencer Bo...</td>\n",
       "      <td>2018-10-05T23:03:00Z</td>\n",
       "      <td>{'id': 'cnbc', 'name': 'CNBC'}</td>\n",
       "      <td>Bitcoin is close to bottoming, cryptocurrency ...</td>\n",
       "      <td>https://www.cnbc.com/2018/10/05/bitcoin-is-clo...</td>\n",
       "      <td>https://fm.cnbc.com/applications/cnbc.com/reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lawrence Abrams</td>\n",
       "      <td>Very very quiet week this. Not much new ransom...</td>\n",
       "      <td>Very very quiet week this. Not much new ransom...</td>\n",
       "      <td>2018-10-05T23:02:12Z</td>\n",
       "      <td>{'id': None, 'name': 'Bleepingcomputer.com'}</td>\n",
       "      <td>The Week in Ransomware - October 5th 2018 - Re...</td>\n",
       "      <td>https://www.bleepingcomputer.com/news/security...</td>\n",
       "      <td>https://www.bleepstatic.com/images/news/column...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                            content  \\\n",
       "0   David Hundeyin  Open source browser-based cryptocurrency walle...   \n",
       "1     Chloe Aiello  Bitcoin is close to bottoming, and once it reb...   \n",
       "2  Lawrence Abrams  Very very quiet week this. Not much new ransom...   \n",
       "\n",
       "                                         description           publishedAt  \\\n",
       "0  Open source browser-based cryptocurrency walle...  2018-10-05T23:25:23Z   \n",
       "1  Bitcoin is close to bottoming, said Spencer Bo...  2018-10-05T23:03:00Z   \n",
       "2  Very very quiet week this. Not much new ransom...  2018-10-05T23:02:12Z   \n",
       "\n",
       "                                              source  \\\n",
       "0  {'id': 'crypto-coins-news', 'name': 'Crypto Co...   \n",
       "1                     {'id': 'cnbc', 'name': 'CNBC'}   \n",
       "2       {'id': None, 'name': 'Bleepingcomputer.com'}   \n",
       "\n",
       "                                               title  \\\n",
       "0  Crypto Mining Giant Bitmain Acquires Bitcoin C...   \n",
       "1  Bitcoin is close to bottoming, cryptocurrency ...   \n",
       "2  The Week in Ransomware - October 5th 2018 - Re...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.ccn.com/crypto-mining-giant-bitmai...   \n",
       "1  https://www.cnbc.com/2018/10/05/bitcoin-is-clo...   \n",
       "2  https://www.bleepingcomputer.com/news/security...   \n",
       "\n",
       "                                          urlToImage  \n",
       "0  https://www.ccn.com/wp-content/uploads/2018/07...  \n",
       "1  https://fm.cnbc.com/applications/cnbc.com/reso...  \n",
       "2  https://www.bleepstatic.com/images/news/column...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Write news contents into a new csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0      Open source browser-based cryptocurrency walle...\\n1      Bitcoin is close to bottoming, and once it reb...\\n2      Very very quiet week this. Not much new ransom...\\n3      A recent survey by Fundstrat Global Advisors h...\\n4      Is this the start of a new phase for Bitcoin o...\\n5      On October 5, 2018, an article published by Bl...\\n6      Track the price of your favourite cryptocurren...\\n7      An exchange-traded bitcoin fund (ETF) would si...\\n8      \"It is just not credible that the United State...\\n9      BitGo, a major Palo Alto-based cryptocurrency ...\\n10     Earlier this year, the city of Atlanta was str...\\n11                                                  None\\n12     The Securities and Exchange Commission (SEC) h...\\n13     Scammers are taking advantage of Fortnite \\'s p...\\n14                                                  None\\n15     The story of this week: Crude Oil- Brent makes...\\n16     &lt;iframe style=\"border: none\" src=\"//html5-p...\\n17     Famous rapper Soulja Boy might just be a bitco...\\n18     BLACKSBURG Block.one has quietly moved into it...\\n19     Herstatt risk or settlement risk is the risk t...\\n20     Recent rumors claim that a BlackBerry licensee...\\n21     NEW YORK (Reuters) - Speculators’ net long bet...\\n22     October 5, 2018: Markets opened mixed Friday e...\\n23     After finding much success in their unique “Bu...\\n24     A couple of days ago, we told you that Optiemu...\\n25     Tracklist: 1. No Hook (True Story) 2. Run up a...\\n26     Square ( NYSE:SQ ) management told investors a...\\n27     The market for cryptocurrency may have slowed ...\\n28     Credit Card Giant Chase Jumps Into Tours and A...\\n29     A new survey conducted by Fundstrat Global Adv...\\n                             ...                        \\n951    Google Play Boots 3 Pretend DeepOnion Wallet A...\\n952    Andreessen Horowitz, the prominent Silicon Val...\\n953    Here are the key things you need to know befor...\\n954    The new six-month release cadence of the JDK m...\\n955    Share to facebook Share to twitter Share to li...\\n956    Michael J. Casey is the chairman of CoinDesk\\'s...\\n957    The prices of digital currencies have collapse...\\n958    As Extra Builders Be part ofTurns out No perso...\\n959    I have been Promoting On LocalDeepOnions For A...\\n960    $10,467,596,650.78 For DeepOnion (Investopedia...\\n961    Apart from ThatOther than That I can verify th...\\n962    The DeepOnion Core DNS Seed CoverageThe Design...\\n963    This story was delivered to Business Insider I...\\n964    $5. Blockchain Can Do Even BetterPurchase A De...\\n965    Last week, our server has been attacked by a h...\\n966    Is Narrative Pushed By DeepOnion, Cryptocurren...\\n967    If You need to use A DeepOnion Wallet\\'Darkish ...\\n968    $5. Blockchain Can Do Even BetterThe Disruptiv...\\n969    The Blockchain Weekly Front Page is a CXO leve...\\n970    Australian Dollar Talking Points: Australian i...\\n971    Ransomware has infected computers used by the ...\\n972    A \\'stablecoin\\' is a cryptocurrency that\\'s pric...\\n973    Cyber security sleuths are scrambling to prote...\\n974    We live in a logical world that has rested its...\\n975    Note: This is part 2 in a multi-part interview...\\n976    California Gov. Jerry Brown has signed into la...\\n977    Port of San Diego CEO Randa Coniglio announced...\\n978    Year-to-year, week-to-week, even day-to-day te...\\n979    OCT Corp., headquartered in Yokohama, Japan, Y...\\n980    OCT Corp., headquartered in Yokohama, Japan, Y...\\nName: content, Length: 981, dtype: object'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "content=str(df['content'])\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'].to_csv('newscontent.csv', index_label = False,index=False)\n",
    "# object is type 'str'\n",
    "# need to split str into list, of tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "### print(type(news.content))\n",
    "print(type(news['content'].iloc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981\n",
      "Index(['author', 'content', 'description', 'publishedAt', 'source', 'title',\n",
      "       'url', 'urlToImage'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(news))\n",
    "print(news.keys())\n",
    "news.head()\n",
    "\n",
    "len(news['url'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.2 Extract features native to the articles__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: **get_info**\n",
    "Function extracts variables from dataframe and stores each as a list, returning all of them as a single dataframe.\n",
    "\n",
    "__Note:__ *urlToImage* is not included in this process, as we are uncertain as to the value of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(df):\n",
    "    \"\"\"\n",
    "    Accepts a dataframe of newsAPI articles, and controls all subbordinate functions that preprocess data\n",
    "    \"\"\"\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    author = []\n",
    "    title = []\n",
    "    publisher = []\n",
    "    publish_url = []\n",
    "    timeStamp = []\n",
    "    description = []\n",
    "    \n",
    "    # loop appends rows to respective lists \n",
    "    for col_name in df:\n",
    "        for index in df[col_name]:\n",
    "            if col_name == 'author':\n",
    "                author.append(index)\n",
    "            elif col_name == 'title':\n",
    "                title.append(index)\n",
    "            elif col_name == 'source':\n",
    "                name = index['name']\n",
    "                publisher.append(name)\n",
    "            elif col_name == 'url':\n",
    "                publish_url.append(index)\n",
    "            elif col_name == 'publishedAt':\n",
    "                timeStamp.append(index)\n",
    "            elif col_name == 'description':\n",
    "                description.append(index)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # merge lists and return them as dataframe.\n",
    "    df = pd.DataFrame({'author' : author,\n",
    "                       'title' : title,\n",
    "                       'publisher' : publisher,\n",
    "                       'source_url' : publish_url,\n",
    "                       'timeStamp' : timeStamp,\n",
    "                       'description' : description})\n",
    "    \n",
    "    return(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed newsAPI Read-in Process: \n",
    "##### newsDF contains features extracted from raw newsAPI feed, for a given data range, and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object creation\n",
    "\n",
    "newsDF = get_info(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Dimensions: (981, 6) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>source_url</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Hundeyin</td>\n",
       "      <td>Crypto Mining Giant Bitmain Acquires Bitcoin C...</td>\n",
       "      <td>Crypto Coins News</td>\n",
       "      <td>https://www.ccn.com/crypto-mining-giant-bitmai...</td>\n",
       "      <td>2018-10-05T23:25:23Z</td>\n",
       "      <td>Open source browser-based cryptocurrency walle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chloe Aiello</td>\n",
       "      <td>Bitcoin is close to bottoming, cryptocurrency ...</td>\n",
       "      <td>CNBC</td>\n",
       "      <td>https://www.cnbc.com/2018/10/05/bitcoin-is-clo...</td>\n",
       "      <td>2018-10-05T23:03:00Z</td>\n",
       "      <td>Bitcoin is close to bottoming, said Spencer Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lawrence Abrams</td>\n",
       "      <td>The Week in Ransomware - October 5th 2018 - Re...</td>\n",
       "      <td>Bleepingcomputer.com</td>\n",
       "      <td>https://www.bleepingcomputer.com/news/security...</td>\n",
       "      <td>2018-10-05T23:02:12Z</td>\n",
       "      <td>Very very quiet week this. Not much new ransom...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author                                              title  \\\n",
       "0   David Hundeyin  Crypto Mining Giant Bitmain Acquires Bitcoin C...   \n",
       "1     Chloe Aiello  Bitcoin is close to bottoming, cryptocurrency ...   \n",
       "2  Lawrence Abrams  The Week in Ransomware - October 5th 2018 - Re...   \n",
       "\n",
       "              publisher                                         source_url  \\\n",
       "0     Crypto Coins News  https://www.ccn.com/crypto-mining-giant-bitmai...   \n",
       "1                  CNBC  https://www.cnbc.com/2018/10/05/bitcoin-is-clo...   \n",
       "2  Bleepingcomputer.com  https://www.bleepingcomputer.com/news/security...   \n",
       "\n",
       "              timeStamp                                        description  \n",
       "0  2018-10-05T23:25:23Z  Open source browser-based cryptocurrency walle...  \n",
       "1  2018-10-05T23:03:00Z  Bitcoin is close to bottoming, said Spencer Bo...  \n",
       "2  2018-10-05T23:02:12Z  Very very quiet week this. Not much new ransom...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying correct data extraction\n",
    "print(\"\\nDataFrame Dimensions:\", newsDF.shape, \"\\n\")\n",
    "newsDF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.3 Perform data cleanup and preprocessing.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions perform basic clean up on a dataframe. The purpose is to prepare the file to write-out (csv).  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'None' values\n",
    "def findNone(df):\n",
    "    \"\"\"\n",
    "     Receives pandas datraframe, and removes null entries from author feature\n",
    "    \"\"\"\n",
    "    print(\"Removed 'None' values in author feature...\")\n",
    "    author = df['author']\n",
    "    publisher = df['publisher']\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        if pd.isnull(author.loc[i]):\n",
    "            author.loc[i] = publisher.loc[i]\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove gaps \n",
    "def gapStrip(df):\n",
    "    \"\"\"\n",
    "    Receives pandas dataframe and leading and traling empty space`\n",
    "    \"\"\"\n",
    "    df.columns = map(str.strip, df.columns) \n",
    "    print(\"Removed leading and trailing spaces and tabs...\")\n",
    "    # element-wise operation\n",
    "    f = lambda x: x.strip() if (isinstance(x,str)) else x\n",
    "    df = df.applymap(f)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize time stamps\n",
    "def std_timeStamp(df):\n",
    "    \"\"\"\n",
    "    Receives pandas dataframe and standardizes time stamps \n",
    "    \"\"\"\n",
    "    import datetime\n",
    "    # Check to see time stamps are in zero timezones\n",
    "    print(\"Converted Time Stamps to Desired Standard Formating...\")\n",
    "    for time in df['timeStamp']:\n",
    "        if time.endswith('Z'):\n",
    "            df['timeStamp'] = pd.to_datetime(df['timeStamp'],\n",
    "                                             infer_datetime_format = True,\n",
    "                                             utc = True)                       # returns a type '.Timestamp'\n",
    "            return(df)\n",
    "        else:\n",
    "            print(\"Revisit appropriate variable or function to deal with time zones that are not zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_clean(df):\n",
    "    \"\"\"\n",
    "    Performs Generic Cleanup and Preprocessing on a given dataframe sourced from newsAPI\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = findNone(df)           # removes missing values from author column\n",
    "    temp2 = gapStrip(temp)        # remove leading and trailing white space\n",
    "    temp3 = std_timeStamp(temp2)  # convert time stamps to 'utc' standard\n",
    "    return(temp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 'None' values in author feature...\n",
      "Removed leading and trailing spaces and tabs...\n",
      "Converted Time Stamps to Desired Standard Formating...\n"
     ]
    }
   ],
   "source": [
    "riskEx_df = feature_clean(newsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              author                                              title  \\\n",
       " 0     David Hundeyin  Crypto Mining Giant Bitmain Acquires Bitcoin C...   \n",
       " 1       Chloe Aiello  Bitcoin is close to bottoming, cryptocurrency ...   \n",
       " 2    Lawrence Abrams  The Week in Ransomware - October 5th 2018 - Re...   \n",
       " 3  Osato Avan-Nomayo  Institutional Investors Bullish About Cryptocu...   \n",
       " 4     Vildana Hajric  With its volatility on the decline, is Bitcoin...   \n",
       " \n",
       "               publisher                                         source_url  \\\n",
       " 0     Crypto Coins News  https://www.ccn.com/crypto-mining-giant-bitmai...   \n",
       " 1                  CNBC  https://www.cnbc.com/2018/10/05/bitcoin-is-clo...   \n",
       " 2  Bleepingcomputer.com  https://www.bleepingcomputer.com/news/security...   \n",
       " 3        Bitcoinist.com  https://bitcoinist.com/institutional-investors...   \n",
       " 4           Latimes.com  http://www.latimes.com/business/la-fi-bitcoin-...   \n",
       " \n",
       "                   timeStamp                                        description  \n",
       " 0 2018-10-05 23:25:23+00:00  Open source browser-based cryptocurrency walle...  \n",
       " 1 2018-10-05 23:03:00+00:00  Bitcoin is close to bottoming, said Spencer Bo...  \n",
       " 2 2018-10-05 23:02:12+00:00  Very very quiet week this. Not much new ransom...  \n",
       " 3 2018-10-05 23:00:03+00:00  A recent survey by Fundstrat Global Advisors h...  \n",
       " 4 2018-10-05 22:55:00+00:00  Is this the start of a new phase for Bitcoin o...  ,\n",
       "                  author                                              title  \\\n",
       " 976  Marguerite Reardon  California's 'gold standard' net neutrality be...   \n",
       " 977           Cali Haan  Permalink to Port of San Diego Hit by “Serious...   \n",
       " 978      Katie Richards  Top Marketers Explain How Emerging Tech Will T...   \n",
       " 979         Sys-con.com  Notification: OCTCoin, from OCT Corp., to Be L...   \n",
       " 980         Sys-con.com  Notification: OCTCoin, from OCT Corp., to Be L...   \n",
       " \n",
       "                 publisher                                         source_url  \\\n",
       " 976              Cnet.com  https://www.cnet.com/news/californias-gold-sta...   \n",
       " 977  Crowdfundinsider.com  https://www.crowdfundinsider.com/2018/09/13960...   \n",
       " 978            Adweek.com  http://www.adweek.com/brand-marketing/top-mark...   \n",
       " 979           Sys-con.com               http://news.sys-con.com/node/4324481   \n",
       " 980           Sys-con.com       http://businesswire.sys-con.com/node/4324481   \n",
       " \n",
       "                     timeStamp  \\\n",
       " 976 2018-10-01 00:27:22+00:00   \n",
       " 977 2018-10-01 00:15:00+00:00   \n",
       " 978 2018-10-01 00:00:51+00:00   \n",
       " 979 2018-10-01 00:00:02+00:00   \n",
       " 980 2018-10-01 00:00:02+00:00   \n",
       " \n",
       "                                            description  \n",
       " 976  It's official. California has adopted net neut...  \n",
       " 977  Port of San Diego CEO Randa Coniglio announced...  \n",
       " 978  Year-to-year, week-to-week, even day-to-day te...  \n",
       " 979  OCT Corp., headquartered in Yokohama, Japan, Y...  \n",
       " 980  OCT Corp., headquartered in Yokohama, Japan, Y...  )"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure data was preprocessed\n",
    "riskEx_df.head(5), riskEx_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 981 entries, 0 to 980\n",
      "Data columns (total 6 columns):\n",
      "author         981 non-null object\n",
      "title          981 non-null object\n",
      "publisher      981 non-null object\n",
      "source_url     981 non-null object\n",
      "timeStamp      981 non-null datetime64[ns, UTC]\n",
      "description    979 non-null object\n",
      "dtypes: datetime64[ns, UTC](1), object(5)\n",
      "memory usage: 46.1+ KB\n"
     ]
    }
   ],
   "source": [
    "## Check file size\n",
    "riskEx_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1.4 Write out to csv.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out n-csv-files each with 100 rows. Process is done to reduce computational load\n",
    "riskEx_df.to_csv('rawData.csv', index_label = False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "917"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('rawData.csv')\n",
    "#print(df.info())\n",
    "print(len(df))\n",
    "len(df['description'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ if wanting to create batches of raw data files containing n-articles, use the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def df_to_csvs(df):\n",
    "#    articlesPage = int(100)\n",
    "#    totalArticles = len(df)\n",
    "#    batchSize=round(totalArticles/articlesPage)          # number of rows in single output file\n",
    "        \n",
    "#    for id, df_i in  enumerate(np.array_split(df, batchSize)):\n",
    "#        df_i.to_csv('rawData_{id}.csv'.format(id=id), index_label = False)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **End Data Mining I:** Read-in NewsAPI feed for a given date range\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
